

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Statement of Need &mdash; Scene Synthesizer 1.14.5 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=dce73442" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=59090042"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Installation" href="install.html" />
    <link rel="prev" title="Scene Synthesizer 1.14.5 Documentation" href="../index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Scene Synthesizer
              <img src="../_static/kitchen_sketch_small.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVlabs/scene_synthesizer">GitHub Repository</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Statement of Need</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../concepts/assets.html">Assets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/scenes.html">Scenes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/procedural_assets.html">Procedural Assets &amp; Procedural Scenes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/arranging_objects.html">Object Arrangement</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/articulations.html">Articulations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/exporting.html">Exporting</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../python_api/scene_synthesizer.scene.html">scene_synthesizer.scene</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python_api/scene_synthesizer.assets.html">scene_synthesizer.assets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python_api/scene_synthesizer.procedural_assets.html">scene_synthesizer.procedural_assets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python_api/scene_synthesizer.procedural_scenes.html">scene_synthesizer.procedural_scenes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python_api/scene_synthesizer.utils.html">scene_synthesizer.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python_api/scene_synthesizer.exchange.export.html">scene_synthesizer.exchange.export</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/add_mdl_materials.html">Adding MDL Materials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/isaacsim.html">Using Scenes in Isaac Sim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/isaaclab_rl.html">Using Scenes in Isaac Lab for RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/isaaclab_teleop.html">Using Scenes in Isaac Lab for Teleop</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Scene Synthesizer</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Statement of Need</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/getting_started/statement_of_need.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="statement-of-need">
<h1>Statement of Need<a class="headerlink" href="#statement-of-need" title="Link to this heading"></a></h1>
<p>Simulation is an ever increasing data source for training deep learning models.
In robotics, simulations have been successfully used to learn behaviors such as navigation, walking, flying or manipulation.
The value of data generation in simulation mainly depends on the diversity and scale of scene layouts.
Existing datasets <span id="id1">[<a class="reference internal" href="#id10" title="Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, and Hao Su. PartNet: a large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). June 2019. doi:10.1109/cvpr.2019.00100.">1</a>, <a class="reference internal" href="#id28" title="Alberto Garcia-Garcia, Pablo Martinez-Gonzalez, Sergiu Oprea, John Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez, and Alvaro Jover-Alvarez. The robotrix: an extremely photorealistic and very-large-scale indoor dataset of sequences with robot trajectories and interactions. 2019. URL: https://arxiv.org/abs/1901.06514, arXiv:1901.06514, doi:10.1109/iros.2018.8594495.">2</a>, <a class="reference internal" href="#id13" title="Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: a framework for visual object manipulation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2021. doi:10.1109/cvpr46437.2021.00447.">3</a>, <a class="reference internal" href="#id18" title="Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: large-scale simulation of everyday tasks for generalist robots. In Robotics: Science and Systems. 2024.">4</a>]</span> are limited in that regard, whereas purely generative models still lack the ability to create scenes that can be used in physics simulator <span id="id2">[<a class="reference internal" href="#id20" title="Jonas Schult, Sam Tsai, Lukas Höllein, Bichen Wu, Jialiang Wang, Chih-Yao Ma, Kunpeng Li, Xiaofang Wang, Felix Wimbauer, Zijian He, Peizhao Zhang, Bastian Leibe, Peter Vajda, and Ji Hou. Controlroom3d: room generation using semantic proxy rooms. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2024. doi:10.1109/cvpr52733.2024.00593.">5</a>, <a class="reference internal" href="#id32" title="Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. Physcene: physically interactable 3d scene synthesis for embodied ai. 2024. URL: https://arxiv.org/abs/2404.09465, arXiv:2404.09465, doi:10.1109/cvpr52733.2024.01539.">6</a>, <a class="reference internal" href="#id21" title="Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: extracting textured 3d meshes from 2d text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 7909-7920. October 2023. doi:10.1109/iccv51070.2023.00727.">7</a>]</span>.
Other procedural pipelines either focus on learning visual models <span id="id3">[<a class="reference internal" href="#id9" title="Maximilian Denninger, Dominik Winkelbauer, Martin Sundermeyer, Wout Boerdijk, Markus Knauer, Klaus H. Strobl, Matthias Humt, and Rudolph Triebel. Blenderproc2: a procedural pipeline for photorealistic rendering. Journal of Open Source Software, 8(82):4901, 2023. URL: https://doi.org/10.21105/joss.04901, doi:10.21105/joss.04901.">8</a>, <a class="reference internal" href="#id16" title="Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scalable dataset generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2022. doi:10.1109/cvpr52688.2022.00373.">9</a>, <a class="reference internal" href="#id17" title="Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, Alejandro Newell, Hei Law, Ankit Goyal, Kaiyu Yang, and Jia Deng. Infinite photorealistic worlds using procedural generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 12630–12641. 2023. doi:10.1109/cvpr52729.2023.01215.">10</a>]</span>, address specific use-cases such as autonomous driving <span id="id4">[<a class="reference internal" href="#id26" title="Daniel J. Fremont, Edward Kim, Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue, Alberto L. Sangiovanni-Vincentelli, and Sanjit A. Seshia. Scenic: A language for scenario specification and data generation. CoRR, 2020. URL: https://arxiv.org/abs/2010.06580, arXiv:2010.06580.">11</a>, <a class="reference internal" href="#id7" title="Timm Hess, Martin Mundt, Iuliia Pliushch, and Visvanathan Ramesh. A procedural world generation framework for systematic evaluation of continual learning. arXiv preprint arXiv:2106.02585, 2021.">12</a>]</span>, or make it hard to be extended and customized since they are tightly integrated with a particular simulation platform <span id="id5">[<a class="reference internal" href="#id15" title="Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, and Roozbeh Mottaghi. ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. In Advances in Neural Information Processing Systems (NeurIPS). 2022.">13</a>]</span>.
With <cite>scene_synthesizer</cite> we present a library that simplifies the process of writing scene randomizers in Python, with a particular focus on physics simulations for robot manipulation. It is fully simulator-agnostic.</p>
<div class="docutils container" id="id6">
<div role="list" class="citation-list">
<div class="citation" id="id10" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, and Hao Su. PartNet: a large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. June 2019. <a class="reference external" href="https://doi.org/10.1109/cvpr.2019.00100">doi:10.1109/cvpr.2019.00100</a>.</p>
</div>
<div class="citation" id="id28" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">2</a><span class="fn-bracket">]</span></span>
<p>Alberto Garcia-Garcia, Pablo Martinez-Gonzalez, Sergiu Oprea, John Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez, and Alvaro Jover-Alvarez. The robotrix: an extremely photorealistic and very-large-scale indoor dataset of sequences with robot trajectories and interactions. 2019. URL: <a class="reference external" href="https://arxiv.org/abs/1901.06514">https://arxiv.org/abs/1901.06514</a>, <a class="reference external" href="https://arxiv.org/abs/1901.06514">arXiv:1901.06514</a>, <a class="reference external" href="https://doi.org/10.1109/iros.2018.8594495">doi:10.1109/iros.2018.8594495</a>.</p>
</div>
<div class="citation" id="id13" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">3</a><span class="fn-bracket">]</span></span>
<p>Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: a framework for visual object manipulation. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. 2021. <a class="reference external" href="https://doi.org/10.1109/cvpr46437.2021.00447">doi:10.1109/cvpr46437.2021.00447</a>.</p>
</div>
<div class="citation" id="id18" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">4</a><span class="fn-bracket">]</span></span>
<p>Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: large-scale simulation of everyday tasks for generalist robots. In <em>Robotics: Science and Systems</em>. 2024.</p>
</div>
<div class="citation" id="id20" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">5</a><span class="fn-bracket">]</span></span>
<p>Jonas Schult, Sam Tsai, Lukas Höllein, Bichen Wu, Jialiang Wang, Chih-Yao Ma, Kunpeng Li, Xiaofang Wang, Felix Wimbauer, Zijian He, Peizhao Zhang, Bastian Leibe, Peter Vajda, and Ji Hou. Controlroom3d: room generation using semantic proxy rooms. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. 2024. <a class="reference external" href="https://doi.org/10.1109/cvpr52733.2024.00593">doi:10.1109/cvpr52733.2024.00593</a>.</p>
</div>
<div class="citation" id="id32" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">6</a><span class="fn-bracket">]</span></span>
<p>Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. Physcene: physically interactable 3d scene synthesis for embodied ai. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2404.09465">https://arxiv.org/abs/2404.09465</a>, <a class="reference external" href="https://arxiv.org/abs/2404.09465">arXiv:2404.09465</a>, <a class="reference external" href="https://doi.org/10.1109/cvpr52733.2024.01539">doi:10.1109/cvpr52733.2024.01539</a>.</p>
</div>
<div class="citation" id="id21" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">7</a><span class="fn-bracket">]</span></span>
<p>Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: extracting textured 3d meshes from 2d text-to-image models. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 7909–7920. October 2023. <a class="reference external" href="https://doi.org/10.1109/iccv51070.2023.00727">doi:10.1109/iccv51070.2023.00727</a>.</p>
</div>
<div class="citation" id="id9" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">8</a><span class="fn-bracket">]</span></span>
<p>Maximilian Denninger, Dominik Winkelbauer, Martin Sundermeyer, Wout Boerdijk, Markus Knauer, Klaus H. Strobl, Matthias Humt, and Rudolph Triebel. Blenderproc2: a procedural pipeline for photorealistic rendering. <em>Journal of Open Source Software</em>, 8(82):4901, 2023. URL: <a class="reference external" href="https://doi.org/10.21105/joss.04901">https://doi.org/10.21105/joss.04901</a>, <a class="reference external" href="https://doi.org/10.21105/joss.04901">doi:10.21105/joss.04901</a>.</p>
</div>
<div class="citation" id="id16" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">9</a><span class="fn-bracket">]</span></span>
<p>Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scalable dataset generator. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. 2022. <a class="reference external" href="https://doi.org/10.1109/cvpr52688.2022.00373">doi:10.1109/cvpr52688.2022.00373</a>.</p>
</div>
<div class="citation" id="id17" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">10</a><span class="fn-bracket">]</span></span>
<p>Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, Alejandro Newell, Hei Law, Ankit Goyal, Kaiyu Yang, and Jia Deng. Infinite photorealistic worlds using procedural generation. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 12630–12641. 2023. <a class="reference external" href="https://doi.org/10.1109/cvpr52729.2023.01215">doi:10.1109/cvpr52729.2023.01215</a>.</p>
</div>
<div class="citation" id="id26" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">11</a><span class="fn-bracket">]</span></span>
<p>Daniel J. Fremont, Edward Kim, Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue, Alberto L. Sangiovanni-Vincentelli, and Sanjit A. Seshia. Scenic: A language for scenario specification and data generation. <em>CoRR</em>, 2020. URL: <a class="reference external" href="https://arxiv.org/abs/2010.06580">https://arxiv.org/abs/2010.06580</a>, <a class="reference external" href="https://arxiv.org/abs/2010.06580">arXiv:2010.06580</a>.</p>
</div>
<div class="citation" id="id7" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">12</a><span class="fn-bracket">]</span></span>
<p>Timm Hess, Martin Mundt, Iuliia Pliushch, and Visvanathan Ramesh. A procedural world generation framework for systematic evaluation of continual learning. <em>arXiv preprint arXiv:2106.02585</em>, 2021.</p>
</div>
<div class="citation" id="id15" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">13</a><span class="fn-bracket">]</span></span>
<p>Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, and Roozbeh Mottaghi. ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. In <em>Advances in Neural Information Processing Systems (NeurIPS)</em>. 2022.</p>
</div>
</div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../index.html" class="btn btn-neutral float-left" title="Scene Synthesizer 1.14.5 Documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="install.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021-2024, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(250);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>